@TechReport{Goodfellow2014,
  author   = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  title    = {Generative {Adversarial} {Networks}},
  year     = {2014},
  month    = jun,
  note     = {arXiv:1406.2661 [cs, stat] type: article},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  doi      = {10.48550/arXiv.1406.2661},
  file     = {arXiv Fulltext PDF:https\://arxiv.org/pdf/1406.2661.pdf:application/pdf},
  keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/1406.2661},
  urldate  = {2024-04-09},
}

@Article{Kohonen1982,
  author   = {Kohonen, Teuvo},
  journal  = {Biological Cybernetics},
  title    = {Self-organized formation of topologically correct feature maps},
  year     = {1982},
  issn     = {1432-0770},
  month    = jan,
  number   = {1},
  pages    = {59--69},
  volume   = {43},
  abstract = {This work contains a theoretical study and computer simulations of a new self-organizing process. The principal discovery is that in a simple network of adaptive physical elements which receives signals from a primary event space, the signal representations are automatically mapped onto a set of output responses in such a way that the responses acquire the same topological order as that of the primary events. In other words, a principle has been discovered which facilitates the automatic formation of topologically correct maps of features of observable events. The basic self-organizing system is a one- or two-dimensional array of processing units resembling a network of threshold-logic units, and characterized by short-range lateral feedback between neighbouring units. Several types of computer simulations are used to demonstrate the ordering process as well as the conditions under which it fails.},
  doi      = {10.1007/BF00337288},
  file     = {Full Text PDF:https\://link.springer.com/content/pdf/10.1007%2FBF00337288.pdf:application/pdf},
  keywords = {Computer Simulation, Primary Event, Output Response, Observable Event, Signal Representation},
  language = {en},
  url      = {https://doi.org/10.1007/BF00337288},
  urldate  = {2024-04-09},
}

@Article{Fukushima1975,
  author     = {Fukushima, Kunihiko},
  journal    = {Biological Cybernetics},
  title      = {Cognitron: {A} self-organizing multilayered neural network},
  year       = {1975},
  issn       = {1432-0770},
  month      = sep,
  number     = {3},
  pages      = {121--136},
  volume     = {20},
  abstract   = {A new hypothesis for the organization of synapses between neurons is proposed: “The synapse from neuron x to neuron y is reinforced when x fires provided that no neuron in the vicinity of y is firing stronger than y”. By introducing this hypothesis, a new algorithm with which a multilayered neural network is effectively organized can be deduced. A self-organizing multilayered neural network, which is named “cognitron”, is constructed following this algorithm, and is simulated on a digital computer. Unlike the organization of a usual brain models such as a three-layered perceptron, the self-organization of a cognitron progresses favorably without having a “teacher” which instructs in all particulars how the individual cells respond. After repetitive presentations of several stimulus patterns, the cognitron is self-organized in such a way that the receptive fields of the cells become relatively larger in a deeper layer. Each cell in the final layer integrates the information from whole parts of the first layer and selectively responds to a specific stimulus pattern or a feature.},
  doi        = {10.1007/BF00342633},
  file       = {:Fukushima1975 - Cognitron_ a Self Organizing Multilayered Neural Network.html:URL},
  keywords   = {Neural Network, Individual Cell, Deep Layer, Receptive Field, Final Layer},
  language   = {en},
  shorttitle = {Cognitron},
  url        = {https://doi.org/10.1007/BF00342633},
  urldate    = {2024-04-09},
}

@Article{Fukushima1980,
  author     = {Fukushima, Kunihiko},
  journal    = {Biological Cybernetics},
  title      = {Neocognitron: {A} self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position},
  year       = {1980},
  issn       = {1432-0770},
  month      = apr,
  number     = {4},
  pages      = {193--202},
  volume     = {36},
  abstract   = {A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by “learning without a teacher”, and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname “neocognitron”. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of “S-cells”, which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of “C-cells” similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any “teacher” during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.},
  doi        = {10.1007/BF00344251},
  file       = {:Fukushima1980 - Neocognitron_ a Self Organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position.html:URL},
  keywords   = {Pattern Recognition, Neural Network Model, Input Layer, Complex Cell, Digital Computer},
  language   = {en},
  shorttitle = {Neocognitron},
  url        = {https://doi.org/10.1007/BF00344251},
  urldate    = {2024-04-09},
}

@Article{Rumelhart1986,
  author    = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  journal   = {Nature},
  title     = {Learning representations by back-propagating errors},
  year      = {1986},
  issn      = {1476-4687},
  month     = oct,
  number    = {6088},
  pages     = {533--536},
  volume    = {323},
  abstract  = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
  copyright = {1986 Springer Nature Limited},
  doi       = {10.1038/323533a0},
  file      = {:Rumelhart1986 - Learning Representations by Back Propagating Errors.html:URL},
  keywords  = {Science, Humanities and Social Sciences, multidisciplinary, Science, multidisciplinary},
  language  = {en},
  publisher = {Nature Publishing Group},
  url       = {https://www.nature.com/articles/323533a0},
  urldate   = {2024-04-09},
}

@Article{McCulloch1943,
  author   = {McCulloch, Warren S. and Pitts, Walter},
  journal  = {The bulletin of mathematical biophysics},
  title    = {A logical calculus of the ideas immanent in nervous activity},
  year     = {1943},
  issn     = {1522-9602},
  month    = dec,
  number   = {4},
  pages    = {115--133},
  volume   = {5},
  abstract = {Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
  doi      = {10.1007/BF02478259},
  file     = {:McCulloch1943 - A Logical Calculus of the Ideas Immanent in Nervous Activity.html:URL},
  keywords = {Nervous Activity, Excitatory Synapse, Inhibitory Synapse, Temporal Summation, Spatial Summation},
  language = {en},
  url      = {https://doi.org/10.1007/BF02478259},
  urldate  = {2024-04-09},
}

@Article{Rosenblatt1958,
  author     = {Rosenblatt, F.},
  journal    = {Psychological Review},
  title      = {The perceptron: {A} probabilistic model for information storage and organization in the brain},
  year       = {1958},
  issn       = {1939-1471},
  number     = {6},
  pages      = {386--408},
  volume     = {65},
  abstract   = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  address    = {US},
  doi        = {10.1037/h0042519},
  keywords   = {Brain, Cognition, Memory, Nervous System},
  publisher  = {American Psychological Association},
  shorttitle = {The perceptron},
}

@Book{Hebb1949,
  author    = {Hebb, D. O.},
  publisher = {Wiley},
  title     = {The organization of behavior; a neuropsychological theory},
  year      = {1949},
  address   = {Oxford, England},
  series    = {The organization of behavior; a neuropsychological theory},
  abstract  = {"This book presents a theory of behavior that is based as far as possible on the physiology of the nervous system, and makes a sedulous attempt to find some community of neurological and psychological conceptions." Using the concept of the reverbatory circuit and the assumption that "some growth process or metabolic change" in neurones takes place as a result of repeated transmission across synapses, perceptual integration is described in terms of "cell-assemblies." Of 11 chapters, 4 are devoted to perceptual problems, 2 to learning, 2 to motivation, and 1 each to emotional disturbances and intelligence. 14-page bibliography. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  pages     = {xix, 335},
}

@Article{Hopfield1982,
  author    = {Hopfield, J J},
  journal   = {Proceedings of the National Academy of Sciences},
  title     = {Neural networks and physical systems with emergent collective computational abilities.},
  year      = {1982},
  issn      = {1091-6490},
  month     = apr,
  number    = {8},
  pages     = {2554--2558},
  volume    = {79},
  doi       = {10.1073/pnas.79.8.2554},
  publisher = {Proceedings of the National Academy of Sciences},
}

@Article{Hubel1959,
  author   = {Hubel, D. H.},
  journal  = {The Journal of Physiology},
  title    = {Single unit activity in striate cortex of unrestrained cats},
  year     = {1959},
  issn     = {0022-3751},
  month    = sep,
  number   = {2},
  pages    = {226--238.2},
  volume   = {147},
  abstract = {Images
null},
  file     = {:hubel_single_1959 - Single Unit Activity in Striate Cortex of Unrestrained Cats.html:URL;:Hubel1959 - Single Unit Activity in Striate Cortex of Unrestrained Cats.pdf:PDF},
  pmcid    = {PMC1357023},
  pmid     = {14403678},
  url      = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1357023/},
  urldate  = {2024-04-09},
}

@Article{Hubel1959a,
  author  = {Hubel, D. H. and Wiesel, T. N.},
  journal = {The Journal of Physiology},
  title   = {Receptive fields of single neurones in the cat's striate cortex},
  year    = {1959},
  issn    = {0022-3751},
  month   = oct,
  number  = {3},
  pages   = {574--591},
  volume  = {148},
  file    = {:Hubel1959a - Receptive Fields of Single Neurones in the Cat's Striate Cortex.html:URL;:Hubel1959a - Receptive Fields of Single Neurones in the Cat's Striate Cortex.pdf:PDF},
  pmcid   = {PMC1363130},
  pmid    = {14403679},
  url     = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1363130/},
  urldate = {2024-04-09},
}

@Comment{jabref-meta: databaseType:bibtex;}
